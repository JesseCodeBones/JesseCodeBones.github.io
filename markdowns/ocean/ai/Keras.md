# Keras Training

## fine tuning
对于数据集中的数据太少而无法从头开始训练完整模型的任务，通常会执行迁移学习。

在深度学习情境中，迁移学习最常见的形式是以下工作流：

从之前训练的模型中获取层。
冻结这些层，以避免在后续训练轮次中破坏它们包含的任何信息。
在已冻结层的顶部添加一些新的可训练层。这些层会学习将旧特征转换为对新数据集的预测。
在您的数据集上训练新层。
最后一个可选步骤是微调，包括解冻上面获得的整个模型（或模型的一部分），然后在新数据上以极低的学习率对该模型进行重新训练。以增量方式使预训练特征适应新数据，有可能实现有意义的改进。  


层和模型还具有布尔特性 trainable。此特性的值可以更改。将 layer.trainable 设置为 False 会将层的所有权重从可训练移至不可训练。这一过程称为“冻结”层：已冻结层的状态在训练期间不会更新（无论是使用 fit() 进行训练，还是使用依赖于 trainable_weights 来应用梯度更新的任何自定义循环进行训练时）。

```python
layer = keras.layers.Dense(3)
layer.build((None, 4))  # Create the weights
layer.trainable = False  # Freeze the layer

print("weights:", len(layer.weights))
print("trainable_weights:", len(layer.trainable_weights))
print("non_trainable_weights:", len(layer.non_trainable_weights))
```

### Loss 训练不下降问题

训练的时候 loss 不下降模型结构问题。当模型结构不好、规模小时，模型对数据的拟合能力不足。  
训练时间问题。不同的模型有不同的计算量，当需要的计算量很大时，耗时也会很大  
权重初始化问题。常用的初始化方案有全零初始化、正态分布初始化和均匀分布初始化等，合适的初始化方案很重要，之前提到过神经网络初始化为0可能会带来的影响  
正则化问题。L1、L2以及Dropout是为了防止过拟合的，当训练集loss下不来时，就要考虑一下是不是正则化过度，导致模型欠拟合了。正则化相关可参考正则化之L1 & L2  
激活函数问题。全连接层多用ReLu，神经网络的输出层会使用sigmoid 或者 softmax。激活函数可参考常用的几个激活函数。在使用Relu激活函数时，当每一个神经元的输入为负时，会使得该神经元输出恒为0，导致失活，由于此时梯度为0，无法恢复。  
优化器问题。优化器一般选取Adam，但是当Adam难以训练时，需要使用如SGD之类的其他优化器。常用优化器可参考机器学习中常用的优化器有哪些？  
学习旅问题。学习率决定了网络的训练速度，但学习率不是越大越好，当网络趋近于收敛时应该选择较小的学习率来保证找到更好的最优点。所以，我们需要手动调整学习率，首先选择一个合适的初始学习率，当训练不动之后，稍微降低学习率。  
梯度消失和爆炸。这时需要考虑激活函数是否合理，网络深度是否合理，可以通过调节sigmoid -> relu，假如残差网络等，相关可参考为什么神经网络会有梯度消失和梯度爆炸问题？如何解决？  
batch size问题。过小，会导致模型损失波动大，难以收敛，过大时，模型前期由于梯度的平均，导致收敛速度过慢。  
数据集问题。（1）数据集未打乱，可能会导致网络在学习过程中产生一定的偏见（2）噪声过多、标注有大量错误时，会导致神经网络难以学到有用的信息，从而出现摇摆不定的情况，噪声、缺失值、异常值（3）数据类别不均衡使得少数类别由于信息量不足，难以学到本质特征，样本不均衡相关可以看样本不均衡及其解决办法。  
特征问题。特征选择不合理，会使网络学习难度增加。  

### dropout机制
https://zhuanlan.zhihu.com/p/410867062

